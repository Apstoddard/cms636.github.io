---
layout: post
published: true
title: Attack on the Provocations
category: commentary
author: Meghana Bhat
---

Bigger Data Are Not Always Better Data

I would argue that a larger sample always leads to stronger conclusions, actually. What seems to be said here is that "twitter users" are being considered like a sampling from "all people," which is not the fault of the use of the data, but rather a fault of the researcher using that data. Twitter is not a random sampling from the world, yes. But neither are most attempts to grab "random" samples of data. Yes, the researcher using twitter data is only making conclusions about twitter users. But with so many data points, these are quite strong conclusions (even if not quite about the matter desired). And he or she should still be free to use that to draw inferences about all peoples' behavior.

Then, about the point that smaller data sets are sometimes better (like when focusing on certain individuals), I ask, how do you locate those individuals? What if you could analyze more individuals in the same depth? Sure, technology isn't quite there yet, but with AI it could reach that point that individual data points can be investigated as if a human were to give it his or her personal attention. The idea of big data is to be able to scale whatever technology can reach.

Not All Data is Equivalent

Context is a huge part of data--obviously you can't isolate data from its context. But that's just like another factor about the data--when did it happen? Where did it happen? Why? If those factors don't stay constant, it's like an experiment where you assume you're only changing one factor, but really, multiple more significant factors are varying. You can't draw good conclusions from an experiment like that. Keeping the other factors as constant as possible will lend strength to the conclusions. Getting more samples of data can also help--even better if you can filter out those where the factors are optimally constant other than the one you are examining.

I feel like much of this article is attacking bad scientific practice. Perhaps researchers are going crazy with big data and ignoring good scientific practice, for this piece to bring up so many instances of such. Perhaps they feel less need to do so with the huge amount of data. Interpretation is still extremely important in big data just as it is in smaller scales of data. In fact, amplifying the data with scale means the interpretation has to be all the more judging, with conclusions being backed up by the strength of the data set. Mistakes are much more deceptive to the general eye.
