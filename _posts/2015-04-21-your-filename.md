---
layout: post
published: true
title: ""
category: updates
---

Sterne and Akiyama present sonification as way to consume and interpret data, much like how visualization and spatialization do the same thing. Sonification, though, uses the unique qualities our ears have to help us interpret data. Our ears can detect tonal shifts and monitor sounds without us having to invest much effort, as opposed to using our eyes, which requires us to focus more. A great example of sonification (that wasn't even mentioned in the reading) is a heart rate monitor. You can probably adequately monitor ~10 people while reading a magazine just by listening to their machines. This is also a task where data visualization wouldn't work very well because you would need to focus your attention on just one person at a time, which is hard to do when there are many people.

Another use of sonification that movies employ is an emotional one. Movie soundtracks aren't there randomly and the type of sound that's playing often coincides with the feelings of the main character. In scary movies there might be a creepy sound playing when the main character looks under the bed. There are also happy and uplifting sounds when the main character
is doing something happy. In both cases, the sound is directly related to the data (namely "scared" and "happy") so the criteria for sonification is upheld.

Sonification is more prevalent than we perceive. It struck me as weird when the authors said that sonification and audification were "new" things and that people don't know how to use them or what to think of them. I think that sonification is so integrated into our lives that we just find it weird to acknowledge it.