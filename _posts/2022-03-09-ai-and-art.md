---
layout: post
published: true
category: commentary
title: AI and art
author: Emma Batson
tags:
  - Week 5
---
## How are musuems using AI

This was a very interesting reading in context of the recent discussion about the limitations of AI for curation work. For instance, this article had a much more positive take on sentiment analysis for interpeting visitor comments, whereas sentiment analysis for understanding and categorizing longform interviews was viewed as less useful. The discrepancy, I think, is that the article doesn't focus on trying to use AI to generate meaning. Instead, the AI it features is echoing visitors' reactions, or answering visitors' questions based on, presumably, aggregation of largely human-generated content, or matching selfies to visually-similar art. Modern AI is extremely good at collecting and sifting through data, and these applications leverage that well. However, it may not be good enough yet to provide true insights into a collection without some human intervention.

## The shape of art history

First, I'm often a machine learning skeptic; I think it's a useful tool that often overhyped. As such, I'm not entirely convinced of this paper's claims, some of which are quite strong compared to the comparatively ambiguous statistical evidence that I think could be plausibly explained by less strong claims. Still, I thought there was a lot that was interesting in this paper. It is very cool that the AI seems to build human-legible axes of, essentially, "painterliness" and "abstractness." But also, they trained the model with style classes that are more or less defined on "painterliness" and "abstractness", so it's a little tough to say this wasn't built into the model. Similarly, it was neat to see a sort of identification of the smooth temporal evolution of style.

However, I'm a little concerned for one about the focus on fairly modern Western art. While this is an understandable limitation of the dataset, it weakens claims like "few underlying factors explain characteristics of different styles." That may be true for American and European art over the past thousand years, but that's quite a small slice of human art to make such a universal claim.

Then taken altogether, these claims imply some weird things, like the seemingly "cyclical" evolution of art. I doubt the next 50 years will produce a lot of art really similar to the Early Renaissance style -- so the model's pseudo-temporal interpretation must be missing something, but what? Can we take the notion of "radial time correlation" seriously when it has explanatory value but no predictive value, or has something simply been overfit?

Overall I feel like there's a lot of suspiciously circular reasoning that I don't know enough about machine learning to untangle. They trained the network on labels produced by art historians, and then got back results that confirmed canonical art history. To me that seems to imply that these labels do in fact encode a lot of information about art, and that ML can decode that information back out, but to me doesn't say anything new about AI or art. Does this support Wolfflin's pairs as a valid theory for all human art, or does it suggest that art historians' labels are largely consistent with Wolfflin's pairs? If we gave a neural network unlabelled paintings and let it come up with its own categories, might it not produce completely different results? That, I think, would really be art history "in the eyes of a machine." 