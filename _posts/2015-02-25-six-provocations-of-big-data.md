---
layout: post
published: true
title: Six Provocations of Big Data
category: commentary
author: Meridian Witt
---

The first three sections of the Boyd’s article caught my attention: the concept of the automation of research, claims of objectivity in big data, and recognizing bigger data is not always better. Studying statistics this semester, I am particularly interested in the analytical approach to big data. Boyd discusses the “temptation” to believe that big data reveals all. Big data risks Type I statistical errors (apophenia): seeing patterns or significance where none exists. This highlights not only the importance of framing and asking the correct questions, but also knowing how to accurately measure what you seek to study and find define a significant result. 

As far as the automation of research is concerned, numbers do not speak for themselves. Numbers can only be extracted if you frame and ask a question that has been designed and crafted with purpose and consequence. Even the way data is held (stored, organized, or held accessible or inaccessible) has lasting effects on the use and questioning of data. Therefore, claims of objectivity in big data should be meet with strigent skepticism. 

In the third section, Boyd also recognizes that bigger data is not always better because it may not be representative. In scientific study currently, inferential statistics and analysis rely on mapping results from a sample to a population or comparing with a population. In big data, we assume this extremely large sample is synonymous with the population. This is simply untrue. As the article mentions, Twitter does not represent all people. We must continue to recognize that not all people have access or proclivities toward “mainstream” social media tools. In fact, the assumption that all people on Twitter relegates the opinions of other people to “otherness” and lacking weight, voice, and importance. Big data, although it holds great variance, does not hold enough variance to simulate an entire population.   

